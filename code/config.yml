model:
  embed_dim: 300
  hidden_dim: 500
  num_layers: 2
training:
  num_epochs: 10
  print_freq: 25
  lr: 0.001
data:
  batch_size: 50000
  words_per_sequence: 200
words:
  vocab_cutoff: 0